{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2 Data Preparation\n",
    "*Alberto Roberto Marinelli, Giacomo Cignoni, Alessandro Bucci*\n",
    "## Importing Libraries\n",
    "First we import the libraries necessary to process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "#from tqdm.notebook import tqdm\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set pandas options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.use_inf_as_na', True) #Set the inf values as pd.NA\n",
    "pd.set_option('max_info_rows',12_000_000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the cleaned dataset adn the users datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv('../dataset/cleaned_joined_tweets.csv', sep=',', index_col=0) # Load cleaned dataset\n",
    "df_users = pd.read_csv('../dataset/cleaned_users.csv', sep=',', index_col=0)  # load users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>bot</th>\n",
       "      <th>created_at</th>\n",
       "      <th>statuses_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2353593986</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-02-22 18:00:42</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2358850842</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>2019-02-26 03:02:32</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137959629</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "      <td>2015-04-30 07:09:56</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>466124818</td>\n",
       "      <td>it</td>\n",
       "      <td>True</td>\n",
       "      <td>2017-01-18 02:49:18</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2571493866</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>2019-06-18 19:30:21</td>\n",
       "      <td>7085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11503</th>\n",
       "      <td>2911861962</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>2019-11-29 13:16:02</td>\n",
       "      <td>1126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11504</th>\n",
       "      <td>1378532629</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-04-27 03:01:58</td>\n",
       "      <td>3024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11505</th>\n",
       "      <td>126984069</td>\n",
       "      <td>es</td>\n",
       "      <td>False</td>\n",
       "      <td>2015-03-29 17:01:24</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11506</th>\n",
       "      <td>2383025796</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-03-13 02:44:13</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11507</th>\n",
       "      <td>933183398</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-11-09 23:24:16</td>\n",
       "      <td>5279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11508 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id lang    bot           created_at  statuses_count\n",
       "0      2353593986   en   True  2019-02-22 18:00:42              76\n",
       "1      2358850842   en  False  2019-02-26 03:02:32              54\n",
       "2       137959629   en   True  2015-04-30 07:09:56               3\n",
       "3       466124818   it   True  2017-01-18 02:49:18              50\n",
       "4      2571493866   en  False  2019-06-18 19:30:21            7085\n",
       "...           ...  ...    ...                  ...             ...\n",
       "11503  2911861962   en  False  2019-11-29 13:16:02            1126\n",
       "11504  1378532629   en  False  2018-04-27 03:01:58            3024\n",
       "11505   126984069   es  False  2015-03-29 17:01:24               6\n",
       "11506  2383025796   en   True  2019-03-13 02:44:13              42\n",
       "11507   933183398   en  False  2017-11-09 23:24:16            5279\n",
       "\n",
       "[11508 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users.rename(columns={'id':'user_id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we cast the columns to the proper datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.created_at = pd.to_datetime(df_cleaned.created_at)\n",
    "df_cleaned.text = df_cleaned.text.astype('string')\n",
    "df_cleaned.lang = pd.Categorical(df_cleaned.lang)\n",
    "\n",
    "df_users.created_at = pd.to_datetime(df_users.created_at)\n",
    "df_users.lang = pd.Categorical(df_users.lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11508 entries, 0 to 11507\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   id              11508 non-null  int64         \n",
      " 1   lang            11508 non-null  category      \n",
      " 2   bot             11508 non-null  bool          \n",
      " 3   created_at      11508 non-null  datetime64[ns]\n",
      " 4   statuses_count  11508 non-null  int64         \n",
      "dtypes: bool(1), category(1), datetime64[ns](1), int64(2)\n",
      "memory usage: 382.8 KB\n"
     ]
    }
   ],
   "source": [
    "df_users.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11277758 entries, 0 to 11277757\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count     Dtype         \n",
      "---  ------           --------------     -----         \n",
      " 0   user_id          11277758 non-null  int64         \n",
      " 1   id               10901728 non-null  float64       \n",
      " 2   retweet_count    11277758 non-null  int64         \n",
      " 3   reply_count      11277758 non-null  int64         \n",
      " 4   favorite_count   11277758 non-null  int64         \n",
      " 5   num_hashtags     11277758 non-null  int64         \n",
      " 6   num_urls         11277758 non-null  int64         \n",
      " 7   num_mentions     11277758 non-null  int64         \n",
      " 8   created_at       11277758 non-null  datetime64[ns]\n",
      " 9   text             10860709 non-null  string        \n",
      " 10  lang             11277758 non-null  category      \n",
      " 11  bot              11277758 non-null  bool          \n",
      " 12  created_at_user  11277758 non-null  object        \n",
      " 13  statuses_count   11277758 non-null  int64         \n",
      "dtypes: bool(1), category(1), datetime64[ns](1), float64(1), int64(8), object(1), string(1)\n",
      "memory usage: 1.1+ GB\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot graphics and correlation map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting indicators\n",
    "Indicators per user covered: \n",
    "\n",
    "* total number of tweets    \n",
    "* average tweet length, \n",
    "* total number of likes, \n",
    "* like ratio per tweet,\n",
    "* counter of tweets outside of possible publishing years\n",
    "* entropy peer user with second wise precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions used to extract indicators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to determine if the tweet is outside of possible publishing years.\n",
    "\n",
    "It is delimited between the 2006, the creation year of twitter and 2020, the year of the data crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = np.datetime64('2006-07-15 00:00:00') # Twitter creation year, used to check if a tweet is published after the creation of twitter\n",
    "max_date = np.datetime64('2020-12-31 23:59:59') # Data scraping year, used to check if a tweet is published before the data scraping\n",
    "\n",
    "def get_tweet_outside_of_possible_publishing_years(tweet):\n",
    "    if tweet.created_at < min_date or tweet.created_at > max_date:  # if the tweet is outside of the publishable period of time\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that returns the number of likes of a specific tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_likes(tweet):\n",
    "    try:\n",
    "        return np.int64(tweet['favorite_count'])\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that calculates average attribute value for user (es: average number of hashtags in a tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_attributes(df, df_indicators): \n",
    "    df_avg = df.groupby(by='user_id').mean()\n",
    "\n",
    "    df_avg.rename(columns = lambda col : str(col)+'_avg', inplace=True)\n",
    "    #df_avg.info()\n",
    "\n",
    "    df_avg.reset_index(inplace=True)\n",
    "    \n",
    "    return df_indicators.merge(df_avg, on='user_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_attributes(df, df_indicators):\n",
    "    df_sum = df.groupby(by='user_id').sum()\n",
    "\n",
    "    df_sum.rename(columns = lambda col : str(col)+'_sum', inplace=True)\n",
    "    #df_sum.info()\n",
    "\n",
    "    df_sum.reset_index(inplace=True)\n",
    "    \n",
    "    return df_indicators.merge(df_sum, on='user_id', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that returns 1 if the tweet has text inside it (if the lenght of it is > 0) and 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_with_text_count(tweet):\n",
    "    tweet_len = len(str(tweet['text']))\n",
    "    if tweet_len > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that returns the average tweet lenght of the tweets of a user, is a moving average that is calculated if the tweet has text inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_tweet_len(indicators_dataframe, user_id ,tweet):\n",
    "    tweet_len = len(str(tweet['text'])) # if tweet['text'] is np.nan, the len(str(np.nan)) is 0\n",
    "    previous_avg = indicators_dataframe.at[user_id, 'avg_tweet_len']\n",
    "\n",
    "    if np.isnan(previous_avg): #if the previous avg does not exist it means that the average has yet to be calculated\n",
    "        return tweet_len #it returns 0 if the len is 0, so if the tweet has no text\n",
    "\n",
    "    elif tweet_len > 0: #if the tweet has a text, use it to calculate the avg tweet len\n",
    "        tweet_count = indicators_dataframe.at[user_id, 'tweet_with_text_count']\n",
    "        \n",
    "        # summing the previous average multiplied for n/n+1 with the current tweet length multiplied for\n",
    "        # 1/n+1 gives us the current average where n is the previous tweet count\n",
    "        avg_tweet_len = previous_avg * ((tweet_count - 1) / tweet_count) \\\n",
    "                        + tweet_len * (1 / tweet_count)\n",
    "\n",
    "        return avg_tweet_len\n",
    "    else: #otherwise return the previous avg\n",
    "        return previous_avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that set the indicators for eache using the functions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_indicators(indicators_dataframe, user_id, tweet):\n",
    "    # if a user published a tweet and is not into the dataframe\n",
    "    if user_id not in indicators_dataframe.index:\n",
    "        # tweet count is set to 1 and the average length is the length of the sole tweet published\n",
    "        #Assign to the indicator dataframe\n",
    "        indicators_dataframe.at[user_id, 'tweet_count'] = 1\n",
    "        indicators_dataframe.at[user_id, 'tweet_with_text_count'] = tweet_with_text_count(tweet)\n",
    "        indicators_dataframe.at[user_id, 'total_num_of_likes'] = get_number_of_likes(tweet)\n",
    "        indicators_dataframe.at[user_id, 'avg_tweet_len'] = get_avg_tweet_len(indicators_dataframe, user_id ,tweet)\n",
    "        indicators_dataframe.at[user_id, 'tweet_outside_of_possible_publishing_years'] = get_tweet_outside_of_possible_publishing_years(tweet)\n",
    "\n",
    "    # if a user published a tweet and is into the dataframe\n",
    "    else:\n",
    "        #Update to the indicator dataframe\n",
    "        indicators_dataframe.at[user_id, 'tweet_count'] += 1\n",
    "        indicators_dataframe.at[user_id, 'tweet_with_text_count'] += tweet_with_text_count(tweet)\n",
    "        indicators_dataframe.at[user_id, 'total_num_of_likes'] += get_number_of_likes(tweet)\n",
    "        indicators_dataframe.at[user_id, 'avg_tweet_len'] = get_avg_tweet_len(indicators_dataframe, user_id ,tweet)\n",
    "        indicators_dataframe.at[user_id, 'tweet_outside_of_possible_publishing_years'] += get_tweet_outside_of_possible_publishing_years(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get the ratios using the values of each user, in this case is performed only the like ratio received per tweet of each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratios(indicators_dataframe):\n",
    "    for id_user, user in tqdm(indicators_dataframe.iterrows(),desc='Getting ratios',total=len(indicators_dataframe)):\n",
    "        # tweet count is always >0, so there is no risk of a zero division\n",
    "        user['like_ratio_per_tweet'] = user['total_num_of_likes'] / user['tweet_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy extraction functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_entropy_dict_from_timedeltas is a function that give the dictionary 'dict_of_dict_of_timedeltas', it calculates the entropy of each user.\n",
    "\n",
    "'dict_of_dict_of_timedeltas' is in the form of  n*{user_id : m*{timedelta_i : number of times timedelta_i has been encountered}} where there are n users and m category of timedelta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The threshold is needed to consider a meaningful number of tweets for applying the entropy\n",
    "entropy_threshold = 10\n",
    "\n",
    "def get_entropy_dict_from_timedeltas(dict_of_dict_of_timedeltas):\n",
    "    entropy_dict = dict()\n",
    "    for user_id in tqdm(dict_of_dict_of_timedeltas,desc='Getting user\\'s entropy from timedeltas',total=len(dict_of_dict_of_timedeltas)):\n",
    "        dict_of_timedeltas_of_the_user = dict_of_dict_of_timedeltas[user_id]\n",
    "        total_number_of_timedeltas = sum(dict_of_timedeltas_of_the_user.values()) #the total number of timedeltas appeared\n",
    "        if total_number_of_timedeltas >= entropy_threshold:\n",
    "            entropy = 0. #entropy set to 0.\n",
    "            for timedelta in dict_of_timedeltas_of_the_user:\n",
    "                number_of_timedelta = dict_of_timedeltas_of_the_user[timedelta] #the number of times the unique timedelta has appeared\n",
    "                #The probability of a timedelta to appear is the number of times the unique timedelta has appeared over total number of times timedeltas appeared\n",
    "                entropy -= number_of_timedelta/total_number_of_timedeltas * np.log2(number_of_timedelta/total_number_of_timedeltas) #shannon's entropy\n",
    "\n",
    "            entropy_dict[user_id] = entropy\n",
    "        else:\n",
    "            entropy_dict[user_id] = np.nan\n",
    "    return entropy_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply_time_precision convert the timedelta to the desired precision specified by 'time_precision'.\n",
    "\n",
    "'time_precision' could be: 'seconds', 'minute', '15minutes', 'hour', 'day'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_time_precision(timedelta, time_precision='second'):\n",
    "    if time_precision=='minute':\n",
    "        return timedelta.round(freq='min') # approximate to nearest minute\n",
    "    elif time_precision=='15minutes':\n",
    "        return timedelta.round(freq='15min') # approximate to nearest 15 minutes\n",
    "    elif time_precision=='hour':\n",
    "        return timedelta.round(freq='h') # approximate to nearest hour\n",
    "    elif time_precision=='day': \n",
    "        return timedelta.round(freq='D') # approximate to nearest day\n",
    "    else:\n",
    "        return timedelta.round(freq='s') # # approximate to nearest second (default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_timedelta_list_per_user generate the dictionary 'dict_of_dict_of_timedeltas' to get the entropies from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timedelta_list_per_user(time_precision='second'):\n",
    "    df_cleaned.sort_values(by=\"created_at\",inplace=True) # is needed to be sorted by date in order to be able to subtract the previous date from the current\n",
    "\n",
    "    last_tweet_encountered = dict() #a dict where the key is user_id and the value is the last post datetime64. It is needed to get the timedelta between posts\n",
    "    tweet_timedeltas = dict() #a dict where the key is user_id and the value is a dict containing the timedeltas:number_of_times_timedelta_encountered\n",
    "\n",
    "    for _, tweet in tqdm(df_cleaned.iterrows(),desc='Getting timedeltas list with '+time_precision+' precision', total=len(df_cleaned)): #iterating on rows\n",
    "        try: #if user_id cannot be casted into int64 it skips the tweet\n",
    "            user_id = np.int64(tweet['user_id'])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if user_id not in tweet_timedeltas.keys(): # if user is not into the timedeltas\n",
    "            last_tweet_encountered[user_id] = tweet.created_at #the first datetime is saved\n",
    "            tweet_timedeltas[user_id] = dict() #the dict containing timedelta:number_of_times_timedelta_encountered is created\n",
    "        else:\n",
    "            timedelta = tweet.created_at - last_tweet_encountered[user_id] #subrtacting the previous datetime64 to the current datetime64 gives the timedelta between the 2\n",
    "            timedelta = apply_time_precision(timedelta, time_precision) #approximate the time to the nearest timedelta given the precision\n",
    "            last_tweet_encountered[user_id] = tweet.created_at # the last datetime64 is saved\n",
    "            if timedelta not in tweet_timedeltas[user_id]: # if the timedelta is not in the dict containing timedelta:number_of_times_timedelta_encountered\n",
    "                tweet_timedeltas[user_id][timedelta] = 1 # it is the first timedelta, so it has appeared only 1 time\n",
    "            else:\n",
    "                tweet_timedeltas[user_id][timedelta] += 1 # it has already appeared, so the number of times encountered increases by 1\n",
    "    \n",
    "    return tweet_timedeltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_entropy_over_time returns the dataframe with the user_id and the associated entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy_over_time(time_precision='second'):\n",
    "    #Subtract from each datetime the previous datetime, obtaining the timedeltas.\n",
    "    #Calculate the entropy on those timedeltas (if the timedeltas are the same, the entropy will be lower)\n",
    "    \n",
    "    list_of_timedeltas = get_timedelta_list_per_user(time_precision)\n",
    "    user_entropy_dict = get_entropy_dict_from_timedeltas(list_of_timedeltas) # a dict where the key is user_id and the value is the entropy of the user\n",
    "\n",
    "    df_entropy = pd.DataFrame(columns=['user_id', 'entropy_'+time_precision])\n",
    "\n",
    "    #saving the dict as a dataframe\n",
    "    index = 0\n",
    "    for user_id in tqdm(user_entropy_dict,desc='Converting dict to dataframe',total=len(user_entropy_dict)): \n",
    "        df_entropy.at[index, 'user_id'] = user_id\n",
    "        df_entropy.at[index, 'entropy_'+time_precision] = user_entropy_dict[user_id]\n",
    "        index += 1\n",
    "    \n",
    "    df_entropy['user_id'] = df_entropy['user_id'].astype('Int64')\n",
    "    df_entropy['entropy_'+time_precision] = df_entropy['entropy_'+time_precision].astype('Float64')\n",
    "    \n",
    "    return df_entropy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_indicators_csv returns the dataframe with the users' ids and all the indicators associated to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_tweet_length(df, df_indicators):\n",
    "    df_text_avg_len = df[['user_id', 'text_len']].groupby(by='user_id').mean()\n",
    "    df_text_avg_len.reset_index(inplace=True)\n",
    "    \n",
    "    df_text_avg_len.rename(columns={'text_len':'avg_text_len'}, inplace=True)\n",
    "    return df_indicators.merge(df_text_avg_len, on='user_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tweets_with_text(df, df_indicators):    \n",
    "\n",
    "    df_tweet_with_text_count = df[['user_id', 'text_len']].loc[df.text_len != 0].groupby(by='user_id').count()\n",
    "    df_tweet_with_text_count.reset_index(inplace=True)\n",
    "    df_tweet_with_text_count.rename(columns={'text_len':'tweet_with_text_count'}, inplace=True)\n",
    "\n",
    "    df_indicators_joined = df_indicators.merge(df_tweet_with_text_count, on='user_id', how='left')\n",
    "    df_indicators_joined.tweet_with_text_count.fillna(value=0, inplace=True)\n",
    "    \n",
    "    \n",
    "    return df_indicators_joined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tweet_count(df):\n",
    "    df_count = df[['user_id', 'text']].groupby(by='user_id').count()\n",
    "    df_count.rename(columns={'text':'tweet_count'}, inplace=True)\n",
    "    df_count.reset_index(inplace=True)\n",
    "    \n",
    "    return df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indicators_csv(df_cleaned):\n",
    "    # create a dataframe for storing indicators\n",
    "    #['tweet_count', 'tweet_with_text_count', 'avg_tweet_len', 'tweets_outside_of_possible_publishing_years'])\n",
    "    # setting the index column name to 'id'\n",
    "    df_indicators = extract_tweet_count(df_cleaned)\n",
    "    print('Done extract_tweet_count')\n",
    "    #print('AFTER TWEET COUNT')\n",
    "    #df_indicators.info()\n",
    "\n",
    "    df_cleaned['text_len'] = df_cleaned.text.apply(lambda text_str : len(str(text_str)))\n",
    "\n",
    "    df_indicators = avg_tweet_length(df_cleaned[['user_id', 'text_len']].copy(), df_indicators)\n",
    "    #print('AFTER AVG')\n",
    "    #df_indicators.info()\n",
    "\n",
    "    df_indicators = count_tweets_with_text(df_cleaned[['user_id', 'text_len']].copy(), df_indicators)\n",
    "\n",
    "\n",
    "    print('Done extract_text_indicators')\n",
    "\n",
    "\n",
    "\n",
    "    for col_name in [\"id\", \"created_at\", \"text\", \"lang\", \"bot\", \"created_at_user\", \"statuses_count\", \"text_len\"]:\n",
    "        del df_cleaned[col_name]\n",
    "\n",
    "    df_indicators = avg_attributes(df_cleaned, df_indicators)\n",
    "    print('Done avg_attributes')\n",
    "\n",
    "    df_indicators = sum_attributes(df_cleaned, df_indicators)\n",
    "    print('Done sum_attributes')\n",
    "\n",
    "    return df_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done extract_tweet_count\n",
      "Done extract_text_indicators\n",
      "Done avg_attributes\n",
      "Done sum_attributes\n"
     ]
    }
   ],
   "source": [
    "indicators = get_indicators_csv(df_cleaned.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11508 entries, 0 to 11507\n",
      "Data columns (total 16 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   user_id                11508 non-null  int64  \n",
      " 1   tweet_count            11508 non-null  int64  \n",
      " 2   avg_text_len           11508 non-null  float64\n",
      " 3   tweet_with_text_count  11508 non-null  int64  \n",
      " 4   retweet_count_avg      11508 non-null  float64\n",
      " 5   reply_count_avg        11508 non-null  float64\n",
      " 6   favorite_count_avg     11508 non-null  float64\n",
      " 7   num_hashtags_avg       11508 non-null  float64\n",
      " 8   num_urls_avg           11508 non-null  float64\n",
      " 9   num_mentions_avg       11508 non-null  float64\n",
      " 10  retweet_count_sum      11508 non-null  int64  \n",
      " 11  reply_count_sum        11508 non-null  int64  \n",
      " 12  favorite_count_sum     11508 non-null  int64  \n",
      " 13  num_hashtags_sum       11508 non-null  int64  \n",
      " 14  num_urls_sum           11508 non-null  int64  \n",
      " 15  num_mentions_sum       11508 non-null  int64  \n",
      "dtypes: float64(7), int64(9)\n",
      "memory usage: 1.5 MB\n"
     ]
    }
   ],
   "source": [
    "indicators.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative Version for before function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indicators_csv_2():\n",
    "    # create a dataframe for storing indicators\n",
    "    df_indicators = pd.DataFrame(columns=['tweet_count', 'tweet_with_text_count', 'avg_tweet_len', 'total_num_of_likes', 'like_ratio_per_tweet', 'tweet_outside_of_possible_publishing_years'])\n",
    "    # setting the index column name to 'id'\n",
    "    df_indicators.index.names = ['user_id']\n",
    "    print('Starting extraction of indicators')\n",
    "    # iterating on tweets\n",
    "    for index, tweet in tqdm(df_cleaned.iterrows(), desc='Indicators extraction', total=len(df_cleaned)):\n",
    "        try: #if user_id cannot be casted into int64 it skips the tweet\n",
    "            user_id = np.int64(tweet['user_id'])\n",
    "        except:\n",
    "            continue\n",
    "        set_indicators(df_indicators, user_id, tweet)\n",
    "\n",
    "    print('Getting ratios')\n",
    "    # ratios\n",
    "    get_ratios(df_indicators)\n",
    "\n",
    "    print('Gettting entropies')\n",
    "    \n",
    "    for time_precision in {'second','minute','15minutes','hour','day'}:\n",
    "        df_entropy = get_entropy_over_time(time_precision=time_precision)\n",
    "        df_indicators = pd.merge(df_indicators, df_entropy, how='inner', left_on='user_id', right_on='user_id')\n",
    "\n",
    "    # save csv with indicators\n",
    "    df_indicators.to_csv('../dataset/indicators.csv')\n",
    "    return df_indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "The reason behind the </> in the output is due to the fact that tqdm was used to get progress bars that can't be seen once the ipynb is reopened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_indicators_csv() missing 1 required positional argument: 'df_cleaned'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22552\\1787006886.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mindicators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_indicators_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: get_indicators_csv() missing 1 required positional argument: 'df_cleaned'"
     ]
    }
   ],
   "source": [
    "indicators = get_indicators_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11508 entries, 0 to 11507\n",
      "Data columns (total 12 columns):\n",
      " #   Column                                      Non-Null Count  Dtype  \n",
      "---  ------                                      --------------  -----  \n",
      " 0   user_id                                     11508 non-null  int64  \n",
      " 1   tweet_count                                 11508 non-null  object \n",
      " 2   tweet_with_text_count                       11508 non-null  object \n",
      " 3   avg_tweet_len                               11508 non-null  object \n",
      " 4   total_num_of_likes                          11508 non-null  object \n",
      " 5   like_ratio_per_tweet                        11508 non-null  object \n",
      " 6   tweet_outside_of_possible_publishing_years  11508 non-null  object \n",
      " 7   entropy_day                                 11085 non-null  Float64\n",
      " 8   entropy_second                              11085 non-null  Float64\n",
      " 9   entropy_minute                              11085 non-null  Float64\n",
      " 10  entropy_15minutes                           11085 non-null  Float64\n",
      " 11  entropy_hour                                11085 non-null  Float64\n",
      "dtypes: Float64(5), int64(1), object(6)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "indicators.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the user dataframe to the user indicator dataframe, dropping the 'tweet_with_text_count' column that was used to calculate the average tweet lenght, converting the dtypes and setting the user_id as the index.\n",
    "\n",
    "Then the final dataframe is saved as a csv because we will load them since we don't want to perform the computation every time since it takes around 4 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_users_indicators = pd.merge(df_users,indicators,left_index=True,right_on='user_id', how='inner')\n",
    "df_users_indicators.drop(columns='tweet_with_text_count',inplace=True)\n",
    "df_users_indicators = df_users_indicators.convert_dtypes()\n",
    "df_users_indicators.set_index('user_id',inplace=True)\n",
    "df_users_indicators.to_csv('../dataset/user_indicators.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the indicators, because we don't want to perform the computation every time since it takes around 4 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_indicators = pd.read_csv('../dataset/user_indicators.csv', sep=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11508 entries, 2353593986 to 933183398\n",
      "Data columns (total 15 columns):\n",
      " #   Column                                      Non-Null Count  Dtype         \n",
      "---  ------                                      --------------  -----         \n",
      " 0   name                                        11507 non-null  string        \n",
      " 1   lang                                        11508 non-null  category      \n",
      " 2   bot                                         11508 non-null  Int64         \n",
      " 3   created_at                                  11508 non-null  datetime64[ns]\n",
      " 4   statuses_count                              11109 non-null  Int64         \n",
      " 5   tweet_count                                 11508 non-null  Int64         \n",
      " 6   avg_tweet_len                               11508 non-null  Float64       \n",
      " 7   total_num_of_likes                          11508 non-null  Int64         \n",
      " 8   like_ratio_per_tweet                        11508 non-null  Float64       \n",
      " 9   tweet_outside_of_possible_publishing_years  11508 non-null  Int64         \n",
      " 10  entropy_day                                 11085 non-null  Float64       \n",
      " 11  entropy_second                              11085 non-null  Float64       \n",
      " 12  entropy_minute                              11085 non-null  Float64       \n",
      " 13  entropy_15minutes                           11085 non-null  Float64       \n",
      " 14  entropy_hour                                11085 non-null  Float64       \n",
      "dtypes: Float64(7), Int64(5), category(1), datetime64[ns](1), string(1)\n",
      "memory usage: 1.5 MB\n"
     ]
    }
   ],
   "source": [
    "df_users_indicators = df_users_indicators.convert_dtypes()\n",
    "df_users_indicators.lang = pd.Categorical(df_users_indicators.lang)\n",
    "df_users_indicators.created_at = pd.to_datetime(df_users_indicators.created_at, errors='coerce')\n",
    "df_users_indicators.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation map on indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we visualize the correlation matrix to highlist the correlation between the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_users_indicators' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22552\\3627865443.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcorr_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_users_indicators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m         \u001b[1;31m# Sample figsize in inches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorr_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtriu_indices_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_users_indicators' is not defined"
     ]
    }
   ],
   "source": [
    "corr_matrix = df_users_indicators.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))         # Sample figsize in inches\n",
    "mask = np.zeros_like(corr_matrix)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Draw the heatmap with the mask\n",
    "sn.heatmap(corr_matrix, annot=True, mask=mask, square=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to use for the clustering:\n",
    "* Entropy 15 minutes\n",
    "* Tweet count\n",
    "* Tweet outside of possible publishing year\n",
    "* Total number of likes\n",
    "* Like ratio per tweet \n",
    "* Avg tweet len\n",
    "\n",
    "The choices were made on the basis that every attribute must not be too much correlated to the others, in order to not have any redundancy in the clustering phase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('dataMiningEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c40c772eaee8ca11ff79c6ae9ceb6f8cf9a67edeabb89d2b5d546b1f7b0ef101"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
