{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2 Data Preparation\n",
    "*Alberto Roberto Marinelli, Giacomo Cignoni, Alessandro Bucci*\n",
    "## Importing Libraries\n",
    "First we import the libraries necessary to process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the cleaned dataset adn the users datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv('../dataset/cleaned_joined_tweets.csv', sep=',', index_col=0) # Load cleaned dataset\n",
    "df_users = pd.read_csv('../dataset/users.csv', sep=',', index_col=0)  # load users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we set pandas options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.use_inf_as_na', True) #Set the inf values as pd.NA\n",
    "pd.set_option('max_info_rows',12_000_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.created_at = pd.to_datetime(df_cleaned.created_at)\n",
    "df_cleaned.text = df_cleaned.text.astype('string')\n",
    "df_cleaned.lang = pd.Categorical(df_cleaned.lang)\n",
    "\n",
    "df_users.name = df_users.name.astype('string')\n",
    "df_users.created_at = pd.to_datetime(df_users.created_at)\n",
    "df_users.lang = pd.Categorical(df_users.lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11508 entries, 2353593986 to 933183398\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   name            11507 non-null  string        \n",
      " 1   lang            11508 non-null  category      \n",
      " 2   bot             11508 non-null  int64         \n",
      " 3   created_at      11508 non-null  datetime64[ns]\n",
      " 4   statuses_count  11109 non-null  float64       \n",
      "dtypes: category(1), datetime64[ns](1), float64(1), int64(1), string(1)\n",
      "memory usage: 462.0 KB\n"
     ]
    }
   ],
   "source": [
    "df_users.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11277758 entries, 0 to 11277757\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count     Dtype         \n",
      "---  ------           --------------     -----         \n",
      " 0   user_id          11277758 non-null  int64         \n",
      " 1   id               10901728 non-null  float64       \n",
      " 2   retweet_count    11277758 non-null  int64         \n",
      " 3   reply_count      11277758 non-null  int64         \n",
      " 4   favorite_count   11277758 non-null  int64         \n",
      " 5   num_hashtags     11277758 non-null  int64         \n",
      " 6   num_urls         11277758 non-null  int64         \n",
      " 7   num_mentions     11277758 non-null  int64         \n",
      " 8   created_at       11277758 non-null  datetime64[ns]\n",
      " 9   text             10860709 non-null  string        \n",
      " 10  lang             11277758 non-null  category      \n",
      " 11  bot              11277758 non-null  bool          \n",
      " 12  created_at_user  11277758 non-null  object        \n",
      " 13  statuses_count   11277758 non-null  int64         \n",
      "dtypes: bool(1), category(1), datetime64[ns](1), float64(1), int64(8), object(1), string(1)\n",
      "memory usage: 1.1+ GB\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot graphics and correlation map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used to extract indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = np.datetime64('2006-07-15 00:00:00') # Twitter creation year, used to check if a tweet is published after the creation of twitter\n",
    "max_date = np.datetime64('2020-12-31 23:59:59') # Data scraping year, used to check if a tweet is published before the data scraping\n",
    "\n",
    "def get_tweet_outside_of_possible_publishing_years(indicators_dataframe, user_id, tweet):\n",
    "    if tweet.created_at < min_date or tweet.created_at > max_date:  # if the tweet is outside of the publishable period of time\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_likes(tweet):\n",
    "    try:\n",
    "        return np.int64(tweet['favorite_count'])\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_with_text_count(indicators_dataframe, user_id, tweet):\n",
    "    tweet_len = len(str(tweet['text']))\n",
    "    if tweet_len > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_tweet_len(indicators_dataframe, user_id ,tweet):\n",
    "    tweet_len = len(str(tweet['text']))\n",
    "    previous_avg = indicators_dataframe.at[user_id, 'avg_tweet_len']\n",
    "\n",
    "    if user_id not in indicators_dataframe.index or np.isnan(previous_avg): #if the user is not in the dataframe or the previous avg does not exist\n",
    "        return tweet_len #it returns 0 if the len is 0, so if the tweet has no text\n",
    "\n",
    "    elif tweet_len > 0: #if the tweet has a text, use it to calculate the avg tweet len\n",
    "        tweet_count = indicators_dataframe.at[user_id, 'tweet_with_text_count']\n",
    "        \n",
    "        # summing the previous average multiplied for n/n+1 with the current tweet length multiplied for\n",
    "        # 1/n+1 gives us the current average where n is the previous tweet count\n",
    "        avg_tweet_len = previous_avg * ((tweet_count - 1) / tweet_count) \\\n",
    "                        + tweet_len * (1 / tweet_count)\n",
    "\n",
    "        return avg_tweet_len\n",
    "    else: #otherwise return the previous avg\n",
    "        return previous_avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_indicators(indicators_dataframe, user_id, tweet):\n",
    "    # if a user published a tweet and is not into the dataframe\n",
    "    if user_id not in indicators_dataframe.index:\n",
    "        # tweet count is set to 1 and the average length is the length of the sole tweet published\n",
    "        #Assign to the indicator dataframe\n",
    "        indicators_dataframe.at[user_id, 'tweet_count'] = 1\n",
    "        indicators_dataframe.at[user_id, 'tweet_with_text_count'] = tweet_with_text_count(indicators_dataframe, user_id, tweet)\n",
    "        indicators_dataframe.at[user_id, 'total_num_of_likes'] = get_number_of_likes(tweet)\n",
    "        indicators_dataframe.at[user_id, 'avg_tweet_len'] = get_avg_tweet_len(indicators_dataframe, user_id ,tweet)\n",
    "        indicators_dataframe.at[user_id, 'tweet_outside_of_possible_publishing_years'] = get_tweet_outside_of_possible_publishing_years(indicators_dataframe,user_id,tweet)\n",
    "\n",
    "    # if a user published a tweet and is into the dataframe\n",
    "    else:\n",
    "        #Update to the indicator dataframe\n",
    "        indicators_dataframe.at[user_id, 'tweet_count'] += 1\n",
    "        indicators_dataframe.at[user_id, 'tweet_with_text_count'] += tweet_with_text_count(indicators_dataframe, user_id, tweet)\n",
    "        indicators_dataframe.at[user_id, 'total_num_of_likes'] += get_number_of_likes(tweet)\n",
    "        indicators_dataframe.at[user_id, 'avg_tweet_len'] = get_avg_tweet_len(indicators_dataframe, user_id ,tweet)\n",
    "        indicators_dataframe.at[user_id, 'tweet_outside_of_possible_publishing_years'] += get_tweet_outside_of_possible_publishing_years(indicators_dataframe,user_id,tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratios(indicators_dataframe):\n",
    "    for id_user, user in tqdm(indicators_dataframe.iterrows(),desc='Getting ratios',total=len(indicators_dataframe)):\n",
    "        # tweet count is always >0, so there is no risk of a zero division\n",
    "        user['like_ratio_per_tweet'] = user['total_num_of_likes'] / user['tweet_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The threshold is needed to consider a meaningful number of tweets for applying the entropy\n",
    "entropy_threshold = 10\n",
    "\n",
    "def get_entropy_dict_from_timedeltas(dict_of_dict_of_timedeltas):\n",
    "    entropy_dict = dict()\n",
    "    for user_id in tqdm(dict_of_dict_of_timedeltas,desc='Getting user\\'s entropy from timedeltas',total=len(dict_of_dict_of_timedeltas)):\n",
    "        dict_of_timedeltas_of_the_user = dict_of_dict_of_timedeltas[user_id]\n",
    "        total_number_of_timedeltas = sum(dict_of_timedeltas_of_the_user.values()) #the total number of timedeltas appeared\n",
    "        if total_number_of_timedeltas >= entropy_threshold:\n",
    "            entropy = 0. #entropy set to 0.\n",
    "            for timedelta in dict_of_timedeltas_of_the_user:\n",
    "                number_of_timedelta = dict_of_timedeltas_of_the_user[timedelta] #the number of times the unique timedelta has appeared\n",
    "                #The probability of a timedelta to appear is the number of times the unique timedelta has appeared over total number of times timedeltas appeared\n",
    "                entropy -= number_of_timedelta/total_number_of_timedeltas * np.log2(number_of_timedelta/total_number_of_timedeltas) #shannon's entropy\n",
    "\n",
    "            entropy_dict[user_id] = entropy\n",
    "        else:\n",
    "            entropy_dict[user_id] = np.nan\n",
    "    return entropy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_time_precision(timedelta, time_precision='second'):\n",
    "    if time_precision=='minute':\n",
    "        return timedelta.round(freq='min') # approximate to nearest minute\n",
    "    elif time_precision=='15minutes':\n",
    "        return timedelta.round(freq='15min') # approximate to nearest 15 minutes\n",
    "    elif time_precision=='hour':\n",
    "        return timedelta.round(freq='h') # approximate to nearest hour\n",
    "    elif time_precision=='day': \n",
    "        return timedelta.round(freq='D') # approximate to nearest day\n",
    "    else:\n",
    "        return timedelta.round(freq='s') # # approximate to nearest second (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timedelta_list_per_user(time_precision='second'):\n",
    "    df_cleaned.sort_values(by=\"created_at\",inplace=True) # is needed to be sorted by date in order to be able to subtract the previous date from the current\n",
    "\n",
    "    last_tweet_encountered = dict() #a dict where the key is user_id and the value is the last post datetime64. It is needed to get the timedelta between posts\n",
    "    tweet_timedeltas = dict() #a dict where the key is user_id and the value is a dict containing the timedeltas:number_of_times_timedelta_encountered\n",
    "\n",
    "    for _, tweet in tqdm(df_cleaned.iterrows(),desc='Getting timedeltas list with '+time_precision+' precision', total=len(df_cleaned)): #iterating on rows\n",
    "        try: #if user_id cannot be casted into int64 it skips the tweet\n",
    "            user_id = np.int64(tweet['user_id'])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if user_id not in tweet_timedeltas.keys(): # if user is not into the timedeltas\n",
    "            last_tweet_encountered[user_id] = tweet.created_at #the first datetime is saved\n",
    "            tweet_timedeltas[user_id] = dict() #the dict containing timedelta:number_of_times_timedelta_encountered is created\n",
    "        else:\n",
    "            timedelta = tweet.created_at - last_tweet_encountered[user_id] #subrtacting the previous datetime64 to the current datetime64 gives the timedelta between the 2\n",
    "            timedelta = apply_time_precision(timedelta, time_precision) #approximate the time to the nearest timedelta given the precision\n",
    "            last_tweet_encountered[user_id] = tweet.created_at # the last datetime64 is saved\n",
    "            if timedelta not in tweet_timedeltas[user_id]: # if the timedelta is not in the dict containing timedelta:number_of_times_timedelta_encountered\n",
    "                tweet_timedeltas[user_id][timedelta] = 1 # it is the first timedelta, so it has appeared only 1 time\n",
    "            else:\n",
    "                tweet_timedeltas[user_id][timedelta] += 1 # it has already appeared, so the number of times encountered increases by 1\n",
    "    \n",
    "    return tweet_timedeltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy_over_time(time_precision='second'):\n",
    "    #Subtract from each datetime the previous datetime, obtaining the timedeltas.\n",
    "    #Calculate the entropy on those timedeltas (if the timedeltas are the same, the entropy will be lower)\n",
    "    \n",
    "    list_of_timedeltas = get_timedelta_list_per_user(time_precision)\n",
    "    user_entropy_dict = get_entropy_dict_from_timedeltas(list_of_timedeltas) # a dict where the key is user_id and the value is the entropy of the user\n",
    "\n",
    "    df_entropy = pd.DataFrame(columns=['user_id', 'entropy_'+time_precision])\n",
    "\n",
    "    #saving the dict as a dataframe\n",
    "    index = 0\n",
    "    for user_id in tqdm(user_entropy_dict,desc='Converting dict to dataframe',total=len(user_entropy_dict)): \n",
    "        df_entropy.at[index, 'user_id'] = user_id\n",
    "        df_entropy.at[index, 'entropy_'+time_precision] = user_entropy_dict[user_id]\n",
    "        index += 1\n",
    "    \n",
    "    df_entropy['user_id'] = df_entropy['user_id'].astype('Int64')\n",
    "    df_entropy['entropy_'+time_precision] = df_entropy['entropy_'+time_precision].astype('Float64')\n",
    "    \n",
    "    return df_entropy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting indicators\n",
    "Indicators per user covered: \n",
    "\n",
    "* total number of tweets    \n",
    "* average tweet length, \n",
    "* total number of likes, \n",
    "* like ratio per tweet,\n",
    "* counter of tweets outside of possible publishing years\n",
    "* entropy peer user with second wise precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indicators_csv():\n",
    "    # create a dataframe for storing indicators\n",
    "    df_indicators = pd.DataFrame(columns=['tweet_count', 'tweet_with_text_count', 'avg_tweet_len', 'total_num_of_likes', 'like_ratio_per_tweet', 'tweet_outside_of_possible_publishing_years'])\n",
    "    # setting the index column name to 'id'\n",
    "    df_indicators.index.names = ['user_id']\n",
    "    print('Starting extraction of indicators')\n",
    "    # iterating on tweets\n",
    "    for index, tweet in tqdm(df_cleaned.iterrows(), desc='Indicators extraction', total=len(df_cleaned)):\n",
    "        try: #if user_id cannot be casted into int64 it skips the tweet\n",
    "            user_id = np.int64(tweet['user_id'])\n",
    "        except:\n",
    "            continue\n",
    "        set_indicators(df_indicators, user_id, tweet)\n",
    "\n",
    "    print('Getting ratios')\n",
    "    # ratios\n",
    "    get_ratios(df_indicators)\n",
    "\n",
    "    print('Gettting entropies')\n",
    "    \n",
    "    for time_precision in {'second','minute','15minutes','hour','day'}:\n",
    "        df_entropy = get_entropy_over_time(time_precision=time_precision)\n",
    "        df_indicators = pd.merge(df_indicators, df_entropy, how='inner', left_on='user_id', right_on='user_id')\n",
    "\n",
    "    # save csv with indicators\n",
    "    df_indicators.to_csv('../dataset/indicators.csv')\n",
    "    return df_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting extraction of indicators\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d3f5a215de4f66a3cc1668014aafbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Indicators extraction:   0%|          | 0/11277758 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting ratios\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1532c2c36dc54ff1bca7229d65da31f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting ratios:   0%|          | 0/11508 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gettting entropies\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff388896c98841ba925baa161fdaf5b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting timedeltas list with minute precision:   0%|          | 0/11277758 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3198b01f56e84e749c618225036e3644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting user's entropy from timedeltas:   0%|          | 0/11508 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9fb478c12b4abc8804336cccbc5a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting dict to dataframe:   0%|          | 0/11508 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd03795c3d44910bb23bb6d06f8dce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting timedeltas list with second precision:   0%|          | 0/11277758 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366a375647784e1f9665dd278de513a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting user's entropy from timedeltas:   0%|          | 0/11508 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49111b891af54a648dfee5392b1f98c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting dict to dataframe:   0%|          | 0/11508 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258f2f466d334232a3268a6e024b613a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting timedeltas list with 15minutes precision:   0%|          | 0/11277758 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2a2f926e7344e6981be44a098df0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting user's entropy from timedeltas:   0%|          | 0/11508 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bcdcb10ae3466fbb5d95a3e50a422b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting dict to dataframe:   0%|          | 0/11508 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae710cbae8b4bb98a5879d741201824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting timedeltas list with day precision:   0%|          | 0/11277758 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfa86697a454c9cb2e92b5471cdba9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting user's entropy from timedeltas:   0%|          | 0/11508 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9a876b32bb41afbf606085993b9bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting dict to dataframe:   0%|          | 0/11508 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75adbcf175714802980564fdd86228cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting timedeltas list with hour precision:   0%|          | 0/11277758 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc13829d63424e569e8ecb8b9320577d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting user's entropy from timedeltas:   0%|          | 0/11508 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0896d9f04f34965b9024f31e0601b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting dict to dataframe:   0%|          | 0/11508 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'user_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11420\\3042212353.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mindicators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_indicators_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_users_indicators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_users\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindicators\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'inner'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrsuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'_indicator'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Kaos1\\anaconda3\\envs\\dataMiningEnv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[0;32m   9098\u001b[0m         \"\"\"\n\u001b[0;32m   9099\u001b[0m         return self._join_compat(\n\u001b[1;32m-> 9100\u001b[1;33m             \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlsuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlsuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrsuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   9101\u001b[0m         )\n\u001b[0;32m   9102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kaos1\\anaconda3\\envs\\dataMiningEnv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_join_compat\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[0;32m   9136\u001b[0m                 \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9137\u001b[0m                 \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlsuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9138\u001b[1;33m                 \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   9139\u001b[0m             )\n\u001b[0;32m   9140\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kaos1\\anaconda3\\envs\\dataMiningEnv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mindicator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m     )\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kaos1\\anaconda3\\envs\\dataMiningEnv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    697\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m         ) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kaos1\\anaconda3\\envs\\dataMiningEnv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1119\u001b[0m                     \u001b[0mjoin_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1120\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                     \u001b[0mleft_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m                     \u001b[0mjoin_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kaos1\\anaconda3\\envs\\dataMiningEnv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1777\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1778\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1779\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1780\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1781\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'user_id'"
     ]
    }
   ],
   "source": [
    "indicators = get_indicators_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_users_indicators = df_users.join(indicators,how='inner',on='user_id',rsuffix='_indicator')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11508 entries, 14265485 to 2200425967\n",
      "Data columns (total 5 columns):\n",
      " #   Column                                      Non-Null Count  Dtype \n",
      "---  ------                                      --------------  ----- \n",
      " 0   tweet_count                                 11508 non-null  object\n",
      " 1   avg_tweet_len                               0 non-null      object\n",
      " 2   total_num_of_likes                          11508 non-null  object\n",
      " 3   like_ratio_per_tweet                        11508 non-null  object\n",
      " 4   tweet_outside_of_possible_publishing_years  120 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 797.5+ KB\n"
     ]
    }
   ],
   "source": [
    "indicators.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation map on indicators"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('dataMiningEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce66766ca3893cb1226d696fde69a866d0cab1d434d25c0e4ac78a42eaec01f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
