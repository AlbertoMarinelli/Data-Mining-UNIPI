{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2 Data Preparation\n",
    "*Alberto Roberto Marinelli, Giacomo Cignoni, Alessandro Bucci*\n",
    "## Importing Libraries\n",
    "First we import the libraries necessary to process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B   C    D\n",
       "0  NaN  2.0 NaN  0.0\n",
       "1  3.0  4.0 NaN  1.0\n",
       "2  NaN  NaN NaN  NaN\n",
       "3  NaN  3.0 NaN  4.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([[np.nan, 2, np.nan, 0],\n",
    "\n",
    "                   [3, 4, np.nan, 1],\n",
    "\n",
    "                   [np.nan, np.nan, np.nan, np.nan],\n",
    "\n",
    "                   [np.nan, 3, np.nan, 4]],\n",
    "\n",
    "                  columns=list(\"ABCD\"))\n",
    "mydict = {0:10,1:11,3:33}\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B     C    D\n",
       "0  NaN  2.0  10.0  0.0\n",
       "1  3.0  4.0  11.0  1.0\n",
       "2  NaN  NaN   NaN  NaN\n",
       "3  NaN  3.0  33.0  4.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.C.fillna(mydict,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the cleaned dataset adn the users datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv('../dataset/cleaned_joined_tweets.csv', sep=',', index_col=0) # Load cleaned dataset\n",
    "df_users = pd.read_csv('../dataset/users.csv', sep=',', index_col=0)  # load users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we set pandas options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.use_inf_as_na', True) #Set the inf values as pd.NA\n",
    "pd.set_option('max_info_rows',12_000_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kaos1\\anaconda3\\envs\\dataMiningEnv\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.created_at = pd.to_datetime(df_cleaned.created_at)\n",
    "df_cleaned.text = df_cleaned.text.astype('string')\n",
    "df_cleaned.lang = pd.Categorical(df_cleaned.lang)\n",
    "\n",
    "df_users.text = df_users.name.astype('string')\n",
    "df_users.created_at = pd.to_datetime(df_users.created_at)\n",
    "df_users.lang = pd.Categorical(df_users.lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11508 entries, 2353593986 to 933183398\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   name            11507 non-null  object        \n",
      " 1   lang            11508 non-null  category      \n",
      " 2   bot             11508 non-null  int64         \n",
      " 3   created_at      11508 non-null  datetime64[ns]\n",
      " 4   statuses_count  11109 non-null  float64       \n",
      "dtypes: category(1), datetime64[ns](1), float64(1), int64(1), object(1)\n",
      "memory usage: 462.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_users.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11277758 entries, 327746321 to 334249560\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count     Dtype         \n",
      "---  ------           --------------     -----         \n",
      " 0   id               10901728 non-null  float64       \n",
      " 1   retweet_count    11277758 non-null  int64         \n",
      " 2   reply_count      11277758 non-null  int64         \n",
      " 3   favorite_count   11277758 non-null  int64         \n",
      " 4   num_hashtags     11277758 non-null  int64         \n",
      " 5   num_urls         11277758 non-null  int64         \n",
      " 6   num_mentions     11277758 non-null  int64         \n",
      " 7   created_at       11277758 non-null  datetime64[ns]\n",
      " 8   text             10860709 non-null  string        \n",
      " 9   lang             11277758 non-null  category      \n",
      " 10  bot              11277758 non-null  bool          \n",
      " 11  created_at_user  11277758 non-null  object        \n",
      " 12  statuses_count   11277758 non-null  int64         \n",
      "dtypes: bool(1), category(1), datetime64[ns](1), float64(1), int64(7), object(1), string(1)\n",
      "memory usage: 1.0+ GB\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot graphics and correlation map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used to extract indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = np.datetime64('2006-07-15 00:00:00') # Twitter creation year, used to check if a tweet is published after the creation of twitter\n",
    "max_date = np.datetime64('2020-12-31 23:59:59') # Data scraping year, used to check if a tweet is published before the data scraping\n",
    "\n",
    "def get_tweet_outside_of_possible_publishing_years(indicators_dataframe, user_id, tweet):\n",
    "    if tweet.created_at < min_date or tweet.created_at > max_date:  # if the tweet is outside of the publishable period of time\n",
    "        tweet_outside_of_range_counter = indicators_dataframe.at[user_id, 'tweet_outside_of_possible_publishing_years']\n",
    "        if tweet_outside_of_range_counter is None:\n",
    "            return 1 # set the counter of tweet_outside_of_possible_publishing_years to 1\n",
    "        else:\n",
    "            return tweet_outside_of_range_counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_likes(tweet):\n",
    "    try:\n",
    "        return np.int64(tweet['favorite_count'])\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_tweet_len(indicators_dataframe, user_id ,tweet):\n",
    "    tweet_len = len(str(tweet['text']))\n",
    "    \n",
    "    if user_id not in indicators_dataframe.index:\n",
    "        avg_tweet_len = tweet_len\n",
    "    else:\n",
    "        previous_tweet_count = indicators_dataframe.at[user_id, 'tweet_count']\n",
    "        previous_avg = indicators_dataframe.at[user_id, 'avg_tweet_len']\n",
    "\n",
    "        # summing the previous average multiplied for n/n+1 with the current tweet length multiplied for\n",
    "        # 1/n+1 gives us the current average where n is the previous tweet count\n",
    "        avg_tweet_len = previous_avg * (previous_tweet_count / (previous_tweet_count + 1)) \\\n",
    "                        + tweet_len * (1 / (previous_tweet_count + 1))\n",
    "    \n",
    "    return avg_tweet_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_indicators(indicators_dataframe, user_id, tweet):\n",
    "    # if a user published a tweet and is not into the dataframe\n",
    "    if user_id not in indicators_dataframe.index:\n",
    "        # tweet count is set to 1 and the average length is the length of the sole tweet published\n",
    "        #Assign to the indicator dataframe\n",
    "        indicators_dataframe.at[user_id, 'tweet_count'] = 1\n",
    "        indicators_dataframe.at[user_id, 'total_num_of_likes'] = get_number_of_likes(tweet)\n",
    "        indicators_dataframe.at[user_id, 'avg_tweet_len'] = get_avg_tweet_len(indicators_dataframe, user_id ,tweet)\n",
    "        indicators_dataframe.at[user_id, 'tweet_outside_of_possible_publishing_years'] = get_tweet_outside_of_possible_publishing_years(indicators_dataframe,user_id,tweet)\n",
    "\n",
    "    # if a user published a tweet and is into the dataframe\n",
    "    else:\n",
    "        #Update to the indicator dataframe\n",
    "        indicators_dataframe.at[user_id, 'tweet_count'] += 1\n",
    "        indicators_dataframe.at[user_id, 'total_num_of_likes'] += get_number_of_likes(tweet)\n",
    "        indicators_dataframe.at[user_id, 'avg_tweet_len'] = get_avg_tweet_len(indicators_dataframe, user_id ,tweet)\n",
    "        indicators_dataframe.at[user_id, 'tweet_outside_of_possible_publishing_years'] = get_tweet_outside_of_possible_publishing_years(indicators_dataframe,user_id,tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratios(indicators_dataframe):\n",
    "    for id_user, user in indicators_dataframe.iterrows():\n",
    "        # tweet count is always >0, so there is no risk of a zero division\n",
    "        user['like_ratio_per_tweet'] = user['total_num_of_likes'] / user['tweet_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The threshold is needed to consider a meaningful number of tweets for applying the entropy\n",
    "entropy_threshold = 10\n",
    "\n",
    "def get_entropy_from_timedeltas(dict_of_timedeltas,user_id):\n",
    "    if len(dict_of_timedeltas[user_id]) >= entropy_threshold:\n",
    "        #The probability of a timedelta to appear is the number of times the unique timedelta has appeared over total number of times timedeltas appeared\n",
    "        total_number_of_timedeltas = sum(dict_of_timedeltas.values()) #the total number of times timedeltas appeared\n",
    "        entropy = 0. #entropy set to 0.\n",
    "        for timedelta in dict_of_timedeltas:\n",
    "            number_of_timedelta = dict_of_timedeltas[timedelta] #the number of times the unique timedelta has appeared\n",
    "            entropy -= number_of_timedelta/total_number_of_timedeltas * np.log2(number_of_timedelta/total_number_of_timedeltas) #shannon's entropy\n",
    "\n",
    "        return entropy\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_time_precision(timedelta, time_precision='second'):\n",
    "    if time_precision=='minute':\n",
    "        return timedelta.round(freq='min') # approximate to nearest minute\n",
    "    elif time_precision=='15minutes':\n",
    "        return timedelta.round(freq='15min') # approximate to nearest 15 minutes\n",
    "    elif time_precision=='hour':\n",
    "        return timedelta.round(freq='h') # approximate to nearest hour\n",
    "    elif time_precision=='day': \n",
    "        return timedelta.round(freq='D') # approximate to nearest day\n",
    "    else:\n",
    "        return timedelta.round(freq='s') # # approximate to nearest second (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timedelta_list_per_user(time_precision='second'):\n",
    "    df_cleaned.sort_values(by=\"created_at\",inplace=True) # is needed to be sorted by date in order to be able to subtract the previous date from the current\n",
    "\n",
    "    last_tweet_encountered = dict() #a dict where the key is user_id and the value is the last post datetime64. It is needed to get the timedelta between posts\n",
    "    tweet_timedeltas = dict() #a dict where the key is user_id and the value is a dict containing the timedeltas:number_of_times_timedelta_encountered\n",
    "\n",
    "    for id_tweet, tweet in df_cleaned.iterrows(): #iterating on rows\n",
    "        try: #if user_id cannot be casted into int64 it skips the tweet\n",
    "            user_id = np.int64(tweet['user_id'])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if user_id not in tweet_timedeltas.keys(): # if user is not into the timedeltas\n",
    "            last_tweet_encountered[user_id] = tweet.created_at #the first datetime is saved\n",
    "            tweet_timedeltas[user_id] = dict() #the dict containing timedelta:number_of_times_timedelta_encountered is created\n",
    "        else:\n",
    "            timedelta = tweet.created_at - last_tweet_encountered[user_id] #subrtacting the previous datetime64 to the current datetime64 gives the timedelta between the 2\n",
    "            timedelta = apply_time_precision(timedelta, time_precision) #approximate the time to the nearest timedelta given the precision\n",
    "            last_tweet_encountered[user_id] = tweet.created_at # the last datetime64 is saved\n",
    "            if timedelta not in tweet_timedeltas[user_id]: # if the timedelta is not in the dict containing timedelta:number_of_times_timedelta_encountered\n",
    "                tweet_timedeltas[user_id][timedelta] = 1 # it is the first timedelta, so it has appeared only 1 time\n",
    "            else:\n",
    "                tweet_timedeltas[user_id][timedelta] += 1 # it has already appeared, so the number of times encountered increases by 1\n",
    "    \n",
    "    return tweet_timedeltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy_over_time(time_precision='second'):\n",
    "    #Subtract from each datetime the previous datetime, obtaining the timedeltas.\n",
    "    #Calculate the entropy on those timedeltas (if the timedeltas are the same, the entropy will be lower)\n",
    "    \n",
    "    user_entropy_list = dict() # The return of the function; a dict where the key is user_id and the value is the entropy of the user\n",
    "    list_of_timedeltas = get_timedelta_list_per_user(time_precision)\n",
    "    \n",
    "    user_entropy_list[user_id] = get_entropy_from_timedeltas(list_of_timedeltas[user_id],user_id) #the entropy is calculated\n",
    "\n",
    "    df_entropy = pd.DataFrame(columns=['user_id', 'entropy'])\n",
    "\n",
    "    #saving the dict as a dataframe\n",
    "    index = 0\n",
    "    for user_id in user_entropy_list: \n",
    "        df_entropy.at[index, 'user_id'] = user_id\n",
    "        df_entropy.at[index, 'entropy'] = user_entropy_list[user_id]\n",
    "        index += 1\n",
    "\n",
    "    df_entropy.to_csv('../dataset/users_entropy.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting indicators\n",
    "Indicators per user covered: \n",
    "\n",
    "* total number of tweets    \n",
    "* average tweet length, \n",
    "* total number of likes, \n",
    "* like ratio per tweet,\n",
    "* counter of tweets outside of possible publishing years\n",
    "* entropy peer user with second wise precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indicators_csv():\n",
    "    # create a dataframe for storing indicators\n",
    "    df_indicators = pd.DataFrame(columns=['tweet_count', 'avg_tweet_len', 'total_num_of_likes', 'like_ratio_per_tweet', 'tweet_outside_of_possible_publishing_years'])\n",
    "    # setting the index column name to 'id'\n",
    "    df_indicators.index.names = ['user_id']\n",
    "\n",
    "    # iterating on tweets\n",
    "    for id_tweet, tweet in df_cleaned.iterrows():\n",
    "        try: #if user_id cannot be casted into int64 it skips the tweet\n",
    "            user_id = np.int64(tweet['user_id'])\n",
    "        except:\n",
    "            continue\n",
    "        set_indicators(df_indicators, user_id, tweet)\n",
    "\n",
    "    # ratios\n",
    "    get_ratios(df_indicators)\n",
    "\n",
    "    df_entropies = pd.read_csv('../dataset/users_entropy.csv')\n",
    "\n",
    "    df_indicators.join(df_entropies, on='user_id',how='inner')\n",
    "\n",
    "    # save csv with indicators\n",
    "    df_indicators.to_csv('../dataset/indicators.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'numpy.ndarray' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_332\\3104604669.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_indicators_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_332\\742820041.py\u001b[0m in \u001b[0;36mget_indicators_csv\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mset_indicators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_indicators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# ratios\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_332\\3609494067.py\u001b[0m in \u001b[0;36mset_indicators\u001b[1;34m(indicators_dataframe, user_id, tweet)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mindicators_dataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'total_num_of_likes'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_number_of_likes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mindicators_dataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'avg_tweet_len'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_avg_tweet_len\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindicators_dataframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_id\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mindicators_dataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tweet_outside_of_possible_publishing_years'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tweet_outside_of_possible_publishing_years\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindicators_dataframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muser_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# if a user published a tweet and is into the dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_332\\1510095058.py\u001b[0m in \u001b[0;36mget_tweet_outside_of_possible_publishing_years\u001b[1;34m(indicators_dataframe, user_id, tweet)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_tweet_outside_of_possible_publishing_years\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindicators_dataframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreated_at\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmin_date\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreated_at\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mmax_date\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# if the tweet is outside of the publishable period of time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mtweet_outside_of_range_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindicators_dataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tweet_outside_of_possible_publishing_years'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtweet_outside_of_range_counter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'numpy.ndarray' and 'str'"
     ]
    }
   ],
   "source": [
    "get_indicators_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation map on indicators"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('dataMiningEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce66766ca3893cb1226d696fde69a866d0cab1d434d25c0e4ac78a42eaec01f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
