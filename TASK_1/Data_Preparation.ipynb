{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2 Data Preparation\n",
    "*Alberto Roberto Marinelli, Giacomo Cignoni, Alessandro Bucci*\n",
    "## Importing Libraries\n",
    "First we import the libraries necessary to process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the cleaned dataset adn the users datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv('../dataset/tweets.csv', sep=',', index_col=0) # Load cleaned dataset\n",
    "df_users = pd.read_csv('../dataset/users.csv', sep=',', index_col=0)  # load users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we set pandas options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.use_inf_as_na', True) #Set the inf values as pd.NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot graphics and correlation map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used to extract indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = np.datetime64('2006-07-15 00:00:00') # Twitter creation year, used to check if a tweet is published after the creation of twitter\n",
    "max_date = np.datetime64('2020-12-31 23:59:59') # Data scraping year, used to check if a tweet is published before the data scraping\n",
    "\n",
    "def get_tweet_outside_of_possible_publishing_years(indicators_dataframe, user_id, tweet):\n",
    "    if tweet.created_at < min_date or tweet.created_at > max_date:  # if the tweet is outside of the publishable period of time\n",
    "        tweet_outside_of_range_counter = indicators_dataframe.at[user_id, 'tweet_outside_of_possible_publishing_years']\n",
    "        if tweet_outside_of_range_counter is None:\n",
    "            return 1 # set the counter of tweet_outside_of_possible_publishing_years to 1\n",
    "        else:\n",
    "            return tweet_outside_of_range_counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_likes(tweet):\n",
    "    try:\n",
    "        return np.int64(tweet['favorite_count'])\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_tweet_len(indicators_dataframe, user_id ,tweet):\n",
    "    tweet_len = len(str(tweet['text']))\n",
    "    \n",
    "    if user_id not in indicators_dataframe.index:\n",
    "        avg_tweet_len = tweet_len\n",
    "    else:\n",
    "        previous_tweet_count = indicators_dataframe.at[user_id, 'tweet_count']\n",
    "        previous_avg = indicators_dataframe.at[user_id, 'avg_tweet_len']\n",
    "\n",
    "        # summing the previous average multiplied for n/n+1 with the current tweet length multiplied for\n",
    "        # 1/n+1 gives us the current average where n is the previous tweet count\n",
    "        avg_tweet_len = previous_avg * (previous_tweet_count / (previous_tweet_count + 1)) \\\n",
    "                        + tweet_len * (1 / (previous_tweet_count + 1))\n",
    "    \n",
    "    return avg_tweet_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_indicators(indicators_dataframe, user_id, tweet):\n",
    "    # if a user published a tweet and is not into the dataframe\n",
    "    if user_id not in indicators_dataframe.index:\n",
    "        # tweet count is set to 1 and the average length is the length of the sole tweet published\n",
    "        #Assign to the indicator dataframe\n",
    "        indicators_dataframe.at[user_id, 'tweet_count'] = 1\n",
    "        indicators_dataframe.at[user_id, 'total_num_of_likes'] = get_number_of_likes(tweet)\n",
    "        indicators_dataframe.at[user_id, 'avg_tweet_len'] = get_avg_tweet_len(indicators_dataframe, user_id ,tweet)\n",
    "        indicators_dataframe.at[user_id, 'tweet_outside_of_possible_publishing_years'] = get_tweet_outside_of_possible_publishing_years(indicators_dataframe,user_id,tweet)\n",
    "\n",
    "    # if a user published a tweet and is into the dataframe\n",
    "    else:\n",
    "        #Update to the indicator dataframe\n",
    "        indicators_dataframe.at[user_id, 'tweet_count'] += 1\n",
    "        indicators_dataframe.at[user_id, 'total_num_of_likes'] += get_number_of_likes(tweet)\n",
    "        indicators_dataframe.at[user_id, 'avg_tweet_len'] = get_avg_tweet_len(indicators_dataframe, user_id ,tweet)\n",
    "        indicators_dataframe.at[user_id, 'tweet_outside_of_possible_publishing_years'] = get_tweet_outside_of_possible_publishing_years(indicators_dataframe,user_id,tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratios(indicators_dataframe):\n",
    "    for id_user, user in indicators_dataframe.iterrows():\n",
    "        # tweet count is always >0, so there is no risk of a zero division\n",
    "        user['like_ratio_per_tweet'] = user['total_num_of_likes'] / user['tweet_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The threshold is needed to consider a meaningful number of tweets for applying the entropy\n",
    "entropy_threshold = 2\n",
    "\n",
    "def get_entropy_from_timedeltas(dict_of_timedeltas,user_id):\n",
    "    if len(dict_of_timedeltas[user_id]) >= entropy_threshold:\n",
    "        #The probability of a timedelta to appear is the number of times the unique timedelta has appeared over total number of times timedeltas appeared\n",
    "        total_number_of_timedeltas = sum(dict_of_timedeltas.values()) #the total number of times timedeltas appeared\n",
    "        entropy = 0. #entropy set to 0.\n",
    "        for timedelta in dict_of_timedeltas:\n",
    "            number_of_timedelta = dict_of_timedeltas[timedelta] #the number of times the unique timedelta has appeared\n",
    "            entropy -= number_of_timedelta/total_number_of_timedeltas * np.log2(number_of_timedelta/total_number_of_timedeltas) #shannon's entropy\n",
    "\n",
    "        return entropy\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 days 04:43:35.000011\n",
      "1 days 00:00:00\n"
     ]
    }
   ],
   "source": [
    "def apply_time_precision(timedelta, time_precision='second'):\n",
    "    if time_precision=='minute':\n",
    "        return timedelta.round(freq='min') # approximate to nearest minute\n",
    "    elif time_precision=='15minutes':\n",
    "        return timedelta.round(freq='15min') # approximate to nearest 15 minutes\n",
    "    elif time_precision=='hour':\n",
    "        return timedelta.round(freq='h') # approximate to nearest hour\n",
    "    elif time_precision=='day': \n",
    "        return timedelta.round(freq='D') # approximate to nearest day\n",
    "    else:\n",
    "        return timedelta.round(freq='s') # # approximate to nearest second (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timedelta_list_per_user(time_precision='second'):\n",
    "    tweets_dataframe = pd.read_csv('./dataset/tweets_sample.csv', sep=',', index_col=0)  # load tweets\n",
    "    tweets_dataframe.created_at = pd.to_datetime(tweets_dataframe.created_at, errors='coerce') # convert created_at to datetime\n",
    "    tweets_dataframe.sort_values(by=\"created_at\",inplace=True) # is needed to be sorted by date in order to be able to subtract the previous date from the current\n",
    "\n",
    "    last_tweet_encountered = dict() #a dict where the key is user_id and the value is the last post datetime64. It is needed to get the timedelta between posts\n",
    "    tweet_timedeltas = dict() #a dict where the key is user_id and the value is a dict containing the timedeltas:number_of_times_timedelta_encountered\n",
    "\n",
    "    for id_tweet, tweet in tweets_dataframe.iterrows(): #iterating on rows\n",
    "        try: #if user_id cannot be casted into int64 it skips the tweet\n",
    "            user_id = np.int64(tweet['user_id'])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if user_id not in tweet_timedeltas.keys(): # if user is not into the timedeltas\n",
    "            last_tweet_encountered[user_id] = tweet.created_at #the first datetime is saved\n",
    "            tweet_timedeltas[user_id] = dict() #the dict containing timedelta:number_of_times_timedelta_encountered is created\n",
    "        else:\n",
    "            timedelta = tweet.created_at - last_tweet_encountered[user_id] #subrtacting the previous datetime64 to the current datetime64 gives the timedelta between the 2\n",
    "            timedelta = apply_time_precision(timedelta, time_precision) #approximate the time to the nearest timedelta given the precision\n",
    "            last_tweet_encountered[user_id] = tweet.created_at # the last datetime64 is saved\n",
    "            if timedelta not in tweet_timedeltas[user_id]: # if the timedelta is not in the dict containing timedelta:number_of_times_timedelta_encountered\n",
    "                tweet_timedeltas[user_id][timedelta] = 1 # it is the first timedelta, so it has appeared only 1 time\n",
    "            else:\n",
    "                tweet_timedeltas[user_id][timedelta] += 1 # it has already appeared, so the number of times encountered increases by 1\n",
    "    \n",
    "    return tweet_timedeltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy_over_time(time_precision='second'):\n",
    "    #Subtract from each datetime the previous datetime, obtaining the timedeltas.\n",
    "    #Calculate the entropy on those timedeltas (if the timedeltas are the same, the entropy will be lower)\n",
    "    \n",
    "    user_entropy_list = dict() # The return of the function; a dict where the key is user_id and the value is the entropy of the user\n",
    "    list_of_timedeltas = get_timedelta_list_per_user(time_precision)\n",
    "    \n",
    "    user_entropy_list[user_id] = get_entropy_from_timedeltas(list_of_timedeltas[user_id],user_id) #the entropy is calculated\n",
    "\n",
    "    df_entropy = pd.DataFrame(columns=['user_id', 'entropy'])\n",
    "\n",
    "    #saving the dict as a dataframe\n",
    "    index = 0\n",
    "    for user_id in user_entropy_list: \n",
    "        df_entropy.at[index, 'user_id'] = user_id\n",
    "        df_entropy.at[index, 'entropy'] = user_entropy_list[user_id]\n",
    "        index += 1\n",
    "\n",
    "    df_entropy.to_csv('../dataset/users_entropy.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting indicators\n",
    "Indicators per user covered: \n",
    "\n",
    "* total number of tweets    \n",
    "* average tweet length, \n",
    "* total number of likes, \n",
    "* like ratio per tweet,\n",
    "* counter of tweets outside of possible publishing years\n",
    "* entropy peer user with second wise precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indicators_csv():\n",
    "    df_tweets = pd.read_csv('../dataset/tweets.csv', sep=',', index_col=0)  # load tweets\n",
    "    # create a dataframe for storing indicators\n",
    "    df_indicators = pd.DataFrame(columns=['tweet_count', 'avg_tweet_len', 'total_num_of_likes', 'like_ratio_per_tweet', 'tweet_outside_of_possible_publishing_years'])\n",
    "    # setting the index column name to 'id'\n",
    "    df_indicators.index.names = ['user_id']\n",
    "\n",
    "    # iterating on tweets\n",
    "    for id_tweet, tweet in df_tweets.iterrows():\n",
    "        try: #if user_id cannot be casted into int64 it skips the tweet\n",
    "            user_id = np.int64(tweet['user_id'])\n",
    "        except:\n",
    "            continue\n",
    "        set_indicators(df_indicators, user_id, tweet)\n",
    "\n",
    "    # ratios\n",
    "    get_ratios(df_indicators)\n",
    "\n",
    "    df_entropies = pd.read_csv('../dataset/users_entropy.csv')\n",
    "\n",
    "    df_indicators.join(df_entropies, on='user_id',how='inner')\n",
    "\n",
    "    # save csv with indicators\n",
    "    df_indicators.to_csv('../dataset/indicators.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation map on indicators"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('dataMiningEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce66766ca3893cb1226d696fde69a866d0cab1d434d25c0e4ac78a42eaec01f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
